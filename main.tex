\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[french,english]{babel}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{natbib}

% Ecriture des maths
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[titlenumbered,ruled,noend,algo2e]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetEndCharOfAlgoLine{}

\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{booktabs}  % for \toprule and \midrule
\usepackage{enumitem}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} % For figures
\graphicspath{{images/}, {prebuiltimages/}, {srcimages/}}
% % added by Quentin for subplots
\usepackage{subcaption}

% Improve hyperref link formating
\usepackage{color}
\definecolor{linkcolor}{RGB}{83,83,182}
\definecolor{citecolor}{RGB}{128,0,128}
\hypersetup{
    colorlinks=true,
    citecolor=citecolor,
    linkcolor=linkcolor,
    urlcolor=linkcolor
}

\usepackage[capitalise,nameinlink]{cleveref} %mm always import this one last, weird conflicts can happen otherwise

\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{natbib}

\setlength{\parskip}{1em}

\Crefname{equation}{Eq.}{Eqs.}
\Crefname{figure}{Fig.}{Figs.}
\Crefname{table}{Tab.}{Tabs.}
\Crefname{algorithm}{Alg.}{Algs.}
\Crefname{appendix}{App.}{Apps.}

\newtheorem{theorem}{Theorem}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*\KL{\mathop{}\!\mathbb{KL}}
\newcommand*\iif{\mathop{}\!\Longleftrightarrow}

\makeatletter
\newcommand{\@giventhatstar}[2]{\left(#1\;\middle|\;#2\right)}
\newcommand{\@giventhatnostar}[3][]{#1(#2\;#1|\;#3#1)}
\newcommand{\giventhat}{\@ifstar\@giventhatstar\@giventhatnostar}
\makeatother

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{\KL\infdivx}

\bibliographystyle{plain}

\begin{document}


\begin{center}
    \LARGE
    Variational Bayesian inference
\end{center}


\section{Variational inference}

Consider the inference problem where we want to approximate some true posterior distribution $p^*(X) = p(X \vert \mathcal{D})$, $\mathcal{D}$ being the dataset.
The basic idea of variational inference is to pick an approximation $q(X)$ to the true distribution from a suited family of distributions, and to make this approximation as close as possible from the true posterior.
This reduces \textbf{inference} to an \textbf{optimization} problem.
Variational inference often gives us the speed benefits of MAP estimation but the statistical benefits of the Bayesian approach.

\section{Mathematical framework and notations}

\subsection{Hypotheses}

A sample $X$ lives in the input space $\mathcal{X}$.
The objective of a generative model is to sample from $P(X)$.
Variational bayesian models have latent variables $z$ living in a latent space $\mathcal{Z}$, with $P(z)$ a distribution we can easily sample from.
We denote by $\Theta$ the parameter space.
%
% VAEs belong to a family of deterministic functions parametrized by $\theta \in \Theta$
% %
% \begin{align}
%     \begin{split}
%     f : \mathcal{Z} \times \Theta &\rightarrow \mathcal{X} \\
%     f : (z; \theta) &\rightarrow x
%     \enspace .
%     \end{split}
% \end{align}
%
% The problem we are solving with VAE is to choose $\theta \in \Theta$ such that $z \sim P(z)$ and $f(z;\theta)$ will be "like" $X$ with high probability.
% More formally, by the law of total probability, we are trying to maximize
%
We use the law of total probability to have an expression of $p(X)$:
%
\begin{equation}
    \label{eq:law_tot_proba}
    p(X) = \int_{\mathcal{Z}}
    \underbrace{p(X \vert z)}_{\textrm{likelihood}}
    \underbrace{p(z)}_\textrm{prior}
    \diff z
    \enspace .
\end{equation}
%
% The parameter vector $\theta \in \Theta$ is estimated using the maximum likelihood principle.
% That is, if the model is likely to produce training set samples, then it is likely to produce similar samples and unlikely to produce dissimilar ones.

\subsection{Mean-field variational Bayes}

Consider a parametrized likelihood function $q_\phi$.
Mean-field variational Bayes uses the Reverse Kullback-Leibler divergence as a metric between two distributions:
%
\begin{equation}
    \label{eq:reverse_kl}
    \infdiv{q_\phi(z \vert X)}{p(z \vert X)} = 
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X)}{p(z \vert X)}
    \enspace .
\end{equation}
%
Although it does not define a distance in some distribution space, it is still a quantity we would like to minimize.
It is the amount of information required to ``distort'' $p(z \vert X)$ into $q_\phi(z \vert X)$, which ideally we would like to be small.
The following form makes it more tractable for an optimization algorithm:
%
\begin{align}
    \infdiv{q_\phi(z \vert X)}{p(z \vert X)} &= 
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X) p(X)}{p(z, X)} \\
    %
    &= \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X)}{p(z, X)}
    +
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log p(X) \\
    %
    &= \log p(X)
    + \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X)}{p(z, X)}
    \enspace .
\end{align} 
%
Therefore,
%
\begin{equation}
    \arg \min_\phi \infdiv{q_\phi(z \vert X)}{p(z \vert X)} 
    = 
    \arg \min_\phi
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X)}{p(z, X)}
    \enspace . 
\end{equation}

The divergence is still not in a tractable form since we need to evaluate $p(z, X)$.

\begin{align}
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X)
    \log
    \frac{q_\phi(z \vert X)}{p(z, X)} &=
    \mathbb{E}_{z \sim q_\phi (z \vert X)}
    \left[
        \log \frac{q_\phi(z \vert X)}{p(z, X)}
    \right] \\
    %
    &= \mathbb{E}_{q_\phi} \left[
        \log q_\phi(z \vert X) - \log p(X, z)
    \right] \\
    &= \mathbb{E}_{q_\phi} \left[
        \log q_\phi(z \vert X) - \log p(X \vert z) - \log p(z)
    \right]
\end{align}
%
While computer software typically \textbf{minimizes} an objective function, we will write the equivalent maximization problem:
%
\begin{align}
    \label{eq:var_low_bound}
    \max_\phi \mathcal{L}(\phi) &=
    - \mathbb{E}_{q_\phi} \left[
        \log q_\phi(z \vert X) - \log p(X \vert z) - \log p(z)
    \right] \\
    &= \mathbb{E}_{q_\phi} \left[
        \log p(X \vert z)
        + \log \frac{p(z)}{q_\phi(z \vert X)}
    \right] \\
    &= \mathbb{E}_{q_\phi} \left[
        \log p(X \vert z)
    \right] +
    \sum_{z \in \mathcal{Z}}
    q_\phi(z \vert X) \log \frac{p(z)}{q_\phi(z \vert X)} \\
    &= \mathbb{E}_{q_\phi} \left[
        \log p(X \vert z)
    \right] -
    \infdiv{q_\phi(z \vert X)}{p(z)}
    \enspace .
\end{align}
%
Plugging back $\mathcal{L}$ into \Cref{eq:reverse_kl}, we get
%
\begin{align}
    \begin{split}
    \infdiv{q_\phi(z \vert X)}{p(z \vert X)} &=
    \log p(X) - \mathcal{L} \\
    %
    \log p(X) &= \mathcal{L} +
    \infdiv{q_\phi(z \vert X)}{p(z \vert X)} 
    \enspace .
    \end{split}
\end{align} 
%
Since $\infdiv{q_\phi(z \vert X)}{p(z \vert X)} \geq 0$, $\log p(X) \geq \mathcal{L}$, making $\mathcal{L}$ a lower bound for $\log p(X)$.
This bound is called the \textbf{variational lower bound} (or \textbf{evidence lower bound}, ELBO).

\subsection{Forward KL vs. Reverse KL}

As stated previously, the Kullback-Leibler (KL) divergence is not symmetric, hence does not define a distance on metric spaces.
Minimizing the forward or the reverse KL will lead to different behavior.
Mean-field variational Bayes uses reverse KL instead of forward KL.
Why?
%
The forward KL divergence reads
%
\begin{equation}
    \label{eq:forward_kl_divergence}
    \infdiv{p(z \vert X)}{q_\phi(z \vert X)} = 
    \sum_{z \in \mathcal{Z}}
    p(z \vert X)
    \log
    \frac{p(z \vert X)}{q_\phi(z \vert X)}
    \enspace .
\end{equation}
%
First, \Cref{eq:forward_kl_divergence} cannot be evaluated since we need to compute $p(z \vert X)$, which we don't know yet.
There is an even more fundamental reason to explain we prefer the reverse divergence to the forward.
\\
\\
As argued by \cite{PML_Murphy}, \Cref{eq:reverse_kl} is infinite if $p(z \vert X) = 0$ and $q_\phi(z \vert X) > 0$.
Thus, to be minimized, \Cref{eq:reverse_kl} will ensure that when $p(z \vert X)= 0$, $q_\phi(z \vert X) = 0$.
The reverse KL is \textbf{zero forcing} for $q_\phi$.
Conversely, \Cref{eq:forward_kl_divergence} is infinite if $q_\phi(z \vert X) = 0$ and $p(z \vert X) > 0$.
To be minimized, \Cref{eq:forward_kl_divergence} will ensure that when $q_\phi(z \vert X) = 0$, $p(z \vert X) > 0$. 
The forward KL is \textbf{zero avoiding} for $q_\phi$.
Hence $q_\phi$ will under-estimate the support of $p$.
\\
\\
When the true distribution is multimodal (which is almost always the case in high-dimensional case), using the forward KL is a bad idea, since the resulting posterior mode/mean will be in a region of low density, right between the two peaks.
In such contexts, the reverse KL is more sensible statistically.

\section{Variational auto-encoders}

Variational autoencoders (VAE) are generative latent variable models.
In statistics, latent variables are hidden variables in the sense that they are not directly observed but are rather inferred through a mathematical model from other variables that are observed.
The goal of a generative model is to construct a model that samples from a (high-dimensional) distribution $P$.
Generative models usually face 3 main issues:
%
\begin{enumerate}
    \item Strong assumptions have to be made on the structure of the data.
    \item Models use severe approximations which lead to suboptimal results.
    \item They have computationally expensive inference procedures (e.g. MCMC).
\end{enumerate}
%
To circumvent these issues, powerful function approximators can be trained via backpropagation with a lot of samples.
These function approximators are typically neural networks, a class of statistical models proved to be universal approximators under mild assumptions.

In VAEs, we assume that samples produced by the model are nearly like $z \in \mathcal{Z}$, that is
%
\begin{equation}
    P(X \vert z, \theta) = \mathcal{N}(X \vert f(z;\theta), \sigma^2 I)
    \enspace ,
\end{equation}
%
with $\sigma$ some hypothesized noise level.
%
\subsection{A surrogate objective}
%
VAE are trained by optimizing $\theta \in \Theta$ via gradient descent to increase $P(X)$ by making $f(z;\theta)$ approach $X$ for some $z \in \mathcal{Z}$.
Intuitively speaking, we are making the training data more likely under the generative model.
This hints at some crucial properties for $f(z;\theta)$ to be optimized: it needs to be computable, continuous and differentiable in $\theta \in \Theta$.
To solve \Cref{eq:law_tot_proba}, there remains 2 problems:
%
\begin{enumerate}
    \item What is $z \in \mathcal{Z}$? How is it defined? What does it represent?
    %
    \item How to evaluate the integral over $\mathcal{Z}$?
\end{enumerate}
%

\subsubsection{Defining latent variables}
%
A lot of information might and need to be encoded in $z \in \mathcal{Z}$, in order to generate a sample.
VAE assume that $z \sim \mathcal{N}(0, I)$.
First, since we do not have a general prior knowledge of how $z$ is distributed we prefer to use a prior maximizing entropy.
Second, by inverse transform sampling, any $d$-dimensional distribution can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function.
In the case of VAE, these functions are typically learnt by gradient descent and approximated by a neural network.
For multi-layer neural networks, one can intuitively think that the first layers are used to map the normally distributed $z$'s to latent values, which can then be mapped to the sampling space.
In other words, the neural network learns the latent structure on its own. Therefore,
%
\begin{equation}
    P(X) = \int_{\mathcal{Z}}
    P(X \vert z; \theta)
    \mathcal{N}(z \vert 0, I)
    \diff z
    \enspace .
\end{equation}

\subsection{Sampling from $\mathcal{Z}$}
%
A naive way to estimate $P(X)$ is to use a Monte-Carlo sampling method, sampling from $P(z)$ and then applying $P(X) \approx \sum_z P(X \vert z; \theta)$.
In practice, this is intractable as we need exponentially more samples as the dimension grows, an instance of the curse of dimensionality.
Intuitively, one might realize that for most $z \in \mathcal{Z}, P(X \lvert z) \approx 0$.
We cannot use a stronger prior on $z$, since it is assumed to be Gaussian.
We need to find a way to select $z$ for which $P(X \lvert z)$ is high and compute $P(X)$ just from those.
The solution is to use a new function $Q(z \lvert X)$ which can take a value $X \in \mathcal{X}$ and outputs a distribution over $\mathcal{Z}$ likely to produce $X \in \mathcal{X}$.
Hopefully, the space of $z$ values likely under $Q$ is much smaller than the space of $z$ values likely under the prior $P(z)$.
\\
\\
Now, that we have defined $Q$, we need to find some mathematical relationship between $P(X)$ and $\mathbb{E}_{z \sim Q}[P(X \vert z)]$.
Remember that for $\theta \in \Theta$ and $z \in \mathcal{Z}$ fixed, $P(X \lvert z)$ is considered a random variable.
Starting from Kullback-Leibler divergence between $P(X \lvert z)$ and $Q(z)$,
%
\begin{equation}
    \KL(Q(z) \lvert\lvert P(z \lvert X)) =
    \mathbb{E}_{z \sim Q}[
        \log Q(z) - \log P(z \lvert X)
    ]
    \enspace .
\end{equation}
%
Using Bayes' rule,
%
\begin{equation}
    \KL(Q(z) \lvert\lvert P(z \lvert X)) =
    \mathbb{E}_{z \sim Q}[
        \log Q(z) - \log P(X \lvert z) - \log P(z)
    ] + \log P(X)
\end{equation}
%
\begin{equation}
    \log P(X) - \KL(Q(z) \vert\vert P(z \vert X)) =
    \mathbb{E}_{z \sim Q}[
        \log P(X \vert z)
        - \KL(Q(z) \vert\vert P(z))
    ]
    \enspace .
\end{equation}
%
Since we are in making $Q$ dependent on $X$, we re-write the equation as
%
\begin{equation}
    \label{eq:vae}
    \underbrace{\log P(X) - \textcolor{blue}{\KL(Q(z \vert X) \vert\vert P(z \vert X))}}_\textrm{a quantity we'd like to maximize} =
    \underbrace{\textcolor{red}{\mathbb{E}_{z \sim Q}[
        \log P(X \vert z)
    ]}
    - \KL(Q(z \vert X) \vert\vert P(z))}_\textrm{a quantity optimizable via SGD}
    \enspace .
\end{equation}
%
\begin{enumerate}
    \item The divergence $\textcolor{blue}{\KL(Q(z \vert X) \vert\vert P(z \vert X))}$ is an error term that makes $Q$ produce $z$ that can reproduce a given $X$.
    It forces $Q(z \vert X)$ to be close to $P(z \vert X)$.
    Assuming an arbitrarily high-capacity model, $Q(z \vert X)$ hopefully matches $P(z \vert X)$.
    $P(z \vert X)$ describes the values of $z \in \mathcal{Z}$ that are likely to give rise to a sample like $X$ in our model.

    \item The term $\textcolor{red}{\mathbb{E}_{z \sim Q}[\log P(X \vert z)]}$ is an encoder/decoder term: $z$ is produced from $X$ via $Q$ and we want to maximize the probability of occurrence of some $X$ given the sampled latent variable $z \in \mathcal{Z}$.
\end{enumerate}
%
Now, we shall give a form to $Q(z \vert X)$.
The usual choice for a VAE is
%
\begin{equation}
    Q(z \vert X) = \mathcal{N}(z \vert \mu(X; \vartheta), \Sigma(X; \vartheta))
    \enspace .
\end{equation}
%
The choice of a Normal distribution is motivated by the fact that $\KL(Q(z \vert X) \vert\vert P(z))$ has a closed-form solution.
%
\begin{equation}
    \KL(Q(z \vert X) \vert\vert P(z)) =
    \frac{1}{2} \left(\mathrm{tr}(\Sigma(X))
    + \mu(X)^{\top}\mu(X)
    - k
    - \log \mathrm{det}(\Sigma(X))
    \right)
\end{equation}
%
Now that we have a closed-form expression for the second term, we need to evaluate $\mathbb{E}_{z \sim Q}[\log P(X \vert z)]$.
We can obtain an approximation of it by sampling from $P(z)$ and compute
%
\begin{equation}
    \nabla_{\theta}
    \left[
        \log P(X \vert z)
        - \KL(Q(z \vert X) \vert\vert P(z))
    \right]
    \enspace .
\end{equation}
%
The issue here is that sampling from $P(z)$ is a \textbf{non-differentiable} operation, which prevents to backpropagate the loss to $\mu(X;\vartheta)$ and $\Sigma(X;\vartheta)$, which parametrizes $Q$.
To circumvent this issue, we use a \textbf{reparametrization trick}. We sample $\epsilon \sim \mathcal{N}(0, I)$, then compute $z = \mu(X;\vartheta) + \Sigma^{â€”1/2}(X;\vartheta) \cdot \epsilon$.

\subsection{Intepreting the objective}

\subsubsection{The error term $\KL(Q(z \vert X) \vert\vert P(z \vert X))$}

This error term is of utmost importance to ensure that our VAE samples from the correct distribution.
%
\begin{theorem}[Convergence to true distribution.]
    Let $P(X)$ be the modelled distribution and $P^*(X)$ the true distribution.
    Let $n$ be the number of samples $z \sim P(z)$, then 
    \begin{equation}
        P(X) \xrightarrow{\enskip \mathcal{L} \enskip} P^*(X) 
        \iif
        \KL(Q(z \vert X) \vert\vert P(z \vert X)) \rightarrow_{n\infty} 0
        \enspace .
    \end{equation}
\end{theorem}
%
Intuitively, $\KL(Q(z \vert X) \vert\vert P(z \vert X))$ pulls our model of $P(X)$ towards parametrization that makes $P(z\vert X)$ Gaussian.

\subsubsection{An interpretation from information theory}
%
\begin{enumerate}
    \item $\KL(Q(z \vert X) \vert\vert P(z))$: expected information required to convert an uninformative sample from $P(z)$ to a sample from $Q(z \vert X)$.
    It is the extra information that we get about $X$ when $z$ comes from $Q(z \vert X)$ instead of $P(z)$.

    \item $P(X \vert z)$: amount of information required to reconstruct $X$ from $z$ with an ideal encoding.
    
    \item $-\log P(X)$: total number of bits required to construct $X$ given an ideal encoding using our model.
\end{enumerate}
%
Therefore, using \Cref{eq:vae}, we notice that minimizing $-\log P(X)$ is a two-step process:
%
\begin{enumerate}
    \item Use some bits to construct $z$ (minimize $\KL(Q(z \vert X) \vert\vert P(z))$)
    
    \item Use some bits to reconstruct $X$ from $z$ (minimize $P(X \vert z)$).
\end{enumerate}
%
Finally, $\KL(Q(z \vert X) \vert\vert P(z))$ is a ``price'' we have to pay for $Q$ being a sub-optimal encoding.

\bibliography{biblio}
\end{document}
%

